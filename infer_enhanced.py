#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•æ ·æœ¬æ¨ç†å¯è§†åŒ–è„šæœ¬ - æ·±å…¥å±•ç¤ºæ¨¡å‹æ¨ç†è¿‡ç¨‹

åŠŸèƒ½ï¼š
1. é€å¸§è§†é¢‘/éŸ³é¢‘ç‰¹å¾å¯è§†åŒ–
2. CAVAå¯¹é½è¿‡ç¨‹åŠ¨æ€å±•ç¤º
3. æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–
4. å¤šæ¨¡æ€èåˆè¿‡ç¨‹
5. é¢„æµ‹ç½®ä¿¡åº¦æ¼”åŒ–
6. å¯è§£é‡Šæ€§åˆ†æ

ä½¿ç”¨æ–¹æ³•ï¼š
    python inference_visualize.py \
        --checkpoint runs/fixed_exp/checkpoints/best_f1.pth \
        --config selfsup_sota.yaml \
        --video path/to/video.mp4 \
        --audio path/to/audio.wav \
        --output ./inference_vis \
        [--sample_idx 0]  # æˆ–ä»æ•°æ®é›†é€‰æ‹©
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import yaml
import warnings

# å¯è§†åŒ–
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec
from matplotlib.animation import FuncAnimation, PillowWriter
import matplotlib.patches as mpatches

# éŸ³è§†é¢‘å¤„ç†
try:
    import cv2
except ImportError:
    print("âš ï¸  OpenCVæœªå®‰è£…ï¼Œè§†é¢‘å¯è§†åŒ–å¯èƒ½å—é™")
    cv2 = None

try:
    import librosa
    import librosa.display
except ImportError:
    print("âš ï¸  librosaæœªå®‰è£…ï¼ŒéŸ³é¢‘å¯è§†åŒ–å¯èƒ½å—é™")
    librosa = None

# å¯¼å…¥æ¨¡å‹
from enhanced_detector import EnhancedAVTopDetector
from dataset import AVFromCSV

sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 150


# é…ç½®Windows/macOS/Linuxå¯ç”¨çš„ä¸­æ–‡å­—ä½“ï¼ˆå¹¶å°½é‡é¿å…ç¼ºå­—å‘Šè­¦ï¼‰
def setup_chinese_font():
    """é…ç½®ä¸­æ–‡å­—ä½“ï¼Œæ”¯æŒWindows/macOS/Linux"""
    import platform
    import matplotlib.font_manager as fm

    system = platform.system()
    tried_paths = []

    def try_add_font(path_list):
        for p in path_list:
            if os.path.exists(p):
                try:
                    fm.fontManager.addfont(p)
                    prop = fm.FontProperties(fname=p)
                    plt.rcParams['font.sans-serif'] = [prop.get_name()]
                    plt.rcParams['font.family'] = 'sans-serif'
                    plt.rcParams['axes.unicode_minus'] = False
                    print(f"âœ“ ä½¿ç”¨å­—ä½“: {prop.get_name()} @ {p}")
                    return True
                except Exception as e:
                    tried_paths.append((p, str(e)))
        return False

    ok = False
    if system == 'Windows':
        win_fonts = os.path.join(os.environ.get('WINDIR', r'C:\Windows'), 'Fonts')
        ok = try_add_font([
            os.path.join(win_fonts, 'msyh.ttc'),       # å¾®è½¯é›…é»‘
            os.path.join(win_fonts, 'msyh.ttf'),
            os.path.join(win_fonts, 'simhei.ttf'),     # é»‘ä½“
            os.path.join(win_fonts, 'simsun.ttc'),     # å®‹ä½“
            os.path.join(win_fonts, 'msyhbd.ttc'),
        ])
    elif system == 'Darwin':
        ok = try_add_font([
            '/System/Library/Fonts/PingFang.ttc',                # è‹¹æ–¹
            '/System/Library/Fonts/STHeiti Light.ttc',
            '/System/Library/Fonts/STHeiti Medium.ttc',
        ])
    else:
        ok = try_add_font([
            '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',    # æ–‡æ³‰é©¿å¾®ç±³é»‘
            '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
            '/usr/share/fonts/opentype/noto/NotoSansCJKsc-Regular.otf',
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',   # å…œåº•ï¼ˆä¸å«CJKå…¨é‡ï¼‰
        ])

    if not ok:
        print("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨ä¸­æ–‡å­—ä½“ï¼Œå¯èƒ½å‡ºç°ç¼ºå­—è­¦å‘Šã€‚å¯åœ¨ç³»ç»Ÿå®‰è£… Noto Sans CJK / å¾®è½¯é›…é»‘ åé‡è¯•ã€‚")

    # å¦‚éœ€é™é»˜â€œGlyph missingâ€æç¤ºï¼Œå¯æ”¾å¼€ä¸‹ä¸€è¡Œï¼ˆä¸å½±å“å›¾åƒå†…å®¹ï¼‰
    # warnings.filterwarnings("ignore", message="Glyph .* missing from current font", category=UserWarning)

setup_chinese_font()


class InferenceVisualizer:
    """å•æ ·æœ¬æ¨ç†å¯è§†åŒ–å™¨"""

    def __init__(
            self,
            model: nn.Module,
            class_names: List[str],
            device: torch.device,
            output_dir: str
    ):
        self.model = model
        self.class_names = class_names
        self.num_classes = len(class_names)
        self.device = device
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / 'frames').mkdir(exist_ok=True)
        (self.output_dir / 'features').mkdir(exist_ok=True)
        (self.output_dir / 'attention').mkdir(exist_ok=True)
        (self.output_dir / 'cava').mkdir(exist_ok=True)
        (self.output_dir / 'fusion').mkdir(exist_ok=True)

        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

    @torch.no_grad()
    def visualize_sample(
            self,
            video: torch.Tensor,
            audio: torch.Tensor,
            label: Optional[int] = None,
            sample_name: str = "sample"
    ):
        """å®Œæ•´å¯è§†åŒ–ä¸€ä¸ªæ ·æœ¬"""
        print("\n" + "=" * 60)
        print(f"ğŸ¬ å¼€å§‹å¯è§†åŒ–æ ·æœ¬: {sample_name}")
        print("=" * 60)

        self.model.eval()

        # ç¡®ä¿batchç»´åº¦
        if video.dim() == 4:  # [T,C,H,W]
            video = video.unsqueeze(0)  # [1,T,C,H,W]
        if audio.dim() == 3:  # [T,M,F]
            audio = audio.unsqueeze(0)  # [1,T,M,F]

        video = video.to(self.device)
        audio = audio.to(self.device)

        # å‰å‘ä¼ æ’­
        outputs = self.model(video, audio, return_aux=True)

        # æå–é¢„æµ‹
        if isinstance(outputs, dict):
            logits = outputs.get('clip_logits', list(outputs.values())[0])
        else:
            logits = outputs

        probs = F.softmax(logits, dim=1).cpu().numpy()[0]
        pred = logits.argmax(dim=1).item()

        print(f"âœ… é¢„æµ‹: {self.class_names[pred]} (ç½®ä¿¡åº¦: {probs[pred]:.3f})")
        if label is not None:
            print(f"   çœŸå®æ ‡ç­¾: {self.class_names[label]}")
            print(f"   {'âœ“ æ­£ç¡®' if pred == label else 'âœ— é”™è¯¯'}")

        # 1. è¾“å…¥æ•°æ®å¯è§†åŒ–
        self._visualize_input(video[0], audio[0], sample_name)

        # 2. é€å¸§ç‰¹å¾æ¼”åŒ–
        self._visualize_temporal_features(video[0], audio[0], outputs, sample_name)

        # 3. CAVAå¯¹é½å¯è§†åŒ–
        if isinstance(outputs, dict):
            self._visualize_cava_alignment(outputs, sample_name)

        # 4. æ³¨æ„åŠ›å›¾
        self._visualize_attention_maps(video[0], audio[0], outputs, sample_name)

        # 5. èåˆè¿‡ç¨‹
        self._visualize_fusion_process(outputs, sample_name)

        # 6. é¢„æµ‹åˆ†æ
        self._visualize_prediction(probs, pred, label, sample_name)

        # 7. ç”Ÿæˆæ€»ç»“å›¾
        self._create_summary_figure(
            video[0], audio[0], outputs, probs, pred, label, sample_name
        )

        print(f"ğŸ’¾ å¯è§†åŒ–å®Œæˆ: {self.output_dir}")

    def _visualize_input(self, video: torch.Tensor, audio: torch.Tensor, name: str):
        """å¯è§†åŒ–è¾“å…¥æ•°æ®"""
        print("\nğŸ“Š 1. è¾“å…¥æ•°æ®å¯è§†åŒ–...")

        video_np = video.cpu().numpy()  # [T, 3, H, W]
        audio_np = audio.cpu().numpy()  # [T, M, F]

        T_v = video_np.shape[0]

        fig = plt.figure(figsize=(20, 10))
        gs = GridSpec(3, T_v, figure=fig)

        # è§†é¢‘å¸§
        for t in range(T_v):
            ax = fig.add_subplot(gs[0, t])
            frame = video_np[t].transpose(1, 2, 0)  # [H,W,3]
            frame = np.clip(frame, 0, 1)
            ax.imshow(frame)
            ax.set_title(f'Frame {t + 1}', fontsize=10)
            ax.axis('off')

        # éŸ³é¢‘å…‰è°±å›¾ï¼ˆæ¯å¸§ï¼‰
        for t in range(T_v):
            ax = fig.add_subplot(gs[1, t])
            spec = audio_np[t]  # [M, F]
            im = ax.imshow(spec, aspect='auto', origin='lower', cmap='viridis')
            ax.set_title(f'Audio {t + 1}', fontsize=10)
            ax.set_ylabel('Mel bins' if t == 0 else '')
            ax.set_xlabel('Frames')
            if t == T_v - 1:
                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

        # éŸ³é¢‘æ³¢å½¢ï¼ˆå…¨å±€ï¼‰
        ax = fig.add_subplot(gs[2, :])
        audio_mean = audio_np.mean(axis=1).flatten()  # å±•å¹³æ‰€æœ‰éŸ³é¢‘
        ax.plot(audio_mean, linewidth=0.5)
        ax.set_xlabel('Time')
        ax.set_ylabel('Amplitude')
        ax.set_title('Audio Waveform (aggregated)', fontweight='bold')
        ax.grid(True, alpha=0.3)

        plt.suptitle(f'è¾“å…¥æ•°æ®: {name}', fontsize=14, fontweight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'frames' / f'{name}_input.png',
                    dpi=150, bbox_inches='tight')
        plt.close()

        print("  âœ“ è¾“å…¥æ•°æ®å·²ä¿å­˜")

    def _visualize_temporal_features(
            self,
            video: torch.Tensor,
            audio: torch.Tensor,
            outputs: Dict,
            name: str
    ):
        """é€å¸§ç‰¹å¾æ¼”åŒ–"""
        print("\nğŸ“ˆ 2. æ—¶åºç‰¹å¾æ¼”åŒ–...")

        if not isinstance(outputs, dict):
            print("  âš ï¸  è¾“å‡ºä¸æ˜¯å­—å…¸ï¼Œè·³è¿‡æ—¶åºç‰¹å¾")
            return

        # æå–æ—¶åºç‰¹å¾
        v_feat = outputs.get('video_proj', outputs.get('video_emb'))
        a_feat = outputs.get('audio_aligned', outputs.get('audio_emb'))

        if v_feat is None or a_feat is None:
            print("  âš ï¸  ç¼ºå°‘æ—¶åºç‰¹å¾")
            return

        v_feat = v_feat[0].cpu().numpy()  # [T, D]
        a_feat = a_feat[0].cpu().numpy()  # [T, D]

        T = min(v_feat.shape[0], a_feat.shape[0])

        fig, axes = plt.subplots(3, 2, figsize=(16, 12))

        # è§†é¢‘ç‰¹å¾çƒ­å›¾
        im1 = axes[0, 0].imshow(v_feat[:T].T, aspect='auto', cmap='viridis')
        axes[0, 0].set_xlabel('Time step')
        axes[0, 0].set_ylabel('Feature dim')
        axes[0, 0].set_title('Video Features', fontweight='bold')
        plt.colorbar(im1, ax=axes[0, 0], fraction=0.046)

        # éŸ³é¢‘ç‰¹å¾çƒ­å›¾
        im2 = axes[0, 1].imshow(a_feat[:T].T, aspect='auto', cmap='viridis')
        axes[0, 1].set_xlabel('Time step')
        axes[0, 1].set_ylabel('Feature dim')
        axes[0, 1].set_title('Audio Features', fontweight='bold')
        plt.colorbar(im2, ax=axes[0, 1], fraction=0.046)

        # ç‰¹å¾èŒƒæ•°æ¼”åŒ–
        v_norm = np.linalg.norm(v_feat[:T], axis=1)
        a_norm = np.linalg.norm(a_feat[:T], axis=1)
        axes[1, 0].plot(range(T), v_norm, 'o-', label='Video', linewidth=2)
        axes[1, 0].plot(range(T), a_norm, 's-', label='Audio', linewidth=2)
        axes[1, 0].set_xlabel('Time step')
        axes[1, 0].set_ylabel('L2 Norm')
        axes[1, 0].set_title('Feature Magnitude', fontweight='bold')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # æ¨¡æ€ç›¸ä¼¼åº¦æ¼”åŒ–
        v_norm_feat = v_feat[:T] / (np.linalg.norm(v_feat[:T], axis=1, keepdims=True) + 1e-8)
        a_norm_feat = a_feat[:T] / (np.linalg.norm(a_feat[:T], axis=1, keepdims=True) + 1e-8)
        similarity = np.sum(v_norm_feat * a_norm_feat, axis=1)

        axes[1, 1].plot(range(T), similarity, 'o-', linewidth=2)
        axes[1, 1].axhline(similarity.mean(), linestyle='--',
                           label=f'Mean={similarity.mean():.3f}')
        axes[1, 1].set_xlabel('Time step')
        axes[1, 1].set_ylabel('Cosine Similarity')
        axes[1, 1].set_title('Modality Similarity Over Time', fontweight='bold')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        # PCAé™ç»´å¯è§†åŒ–
        from sklearn.decomposition import PCA
        combined = np.concatenate([v_feat[:T], a_feat[:T]], axis=0)
        if combined.shape[1] >= 2:
            pca = PCA(n_components=2)
            combined_2d = pca.fit_transform(combined)

            v_2d = combined_2d[:T]
            a_2d = combined_2d[T:]

            axes[2, 0].scatter(v_2d[:, 0], v_2d[:, 1], c=range(T),
                               cmap='Reds', s=100, alpha=0.7, label='Video')
            axes[2, 0].scatter(a_2d[:, 0], a_2d[:, 1], c=range(T),
                               cmap='Blues', s=100, alpha=0.7, marker='s', label='Audio')

            # è¿çº¿æ˜¾ç¤ºæ—¶åº
            axes[2, 0].plot(v_2d[:, 0], v_2d[:, 1], 'r-', alpha=0.3, linewidth=1)
            axes[2, 0].plot(a_2d[:, 0], a_2d[:, 1], 'b-', alpha=0.3, linewidth=1)

            axes[2, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
            axes[2, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
            axes[2, 0].set_title('Feature Trajectory (PCA)', fontweight='bold')
            axes[2, 0].legend()
            axes[2, 0].grid(True, alpha=0.3)

        # ç‰¹å¾ç»Ÿè®¡
        axes[2, 1].axis('off')
        stats_text = f"""
ç‰¹å¾ç»Ÿè®¡ (T={T}):

è§†é¢‘ç‰¹å¾:
  - ç»´åº¦: {v_feat.shape[1]}
  - å‡å€¼: {v_feat.mean():.3f}
  - æ ‡å‡†å·®: {v_feat.std():.3f}
  - èŒƒå›´: [{v_feat.min():.3f}, {v_feat.max():.3f}]

éŸ³é¢‘ç‰¹å¾:
  - ç»´åº¦: {a_feat.shape[1]}
  - å‡å€¼: {a_feat.mean():.3f}
  - æ ‡å‡†å·®: {a_feat.std():.3f}
  - èŒƒå›´: [{a_feat.min():.3f}, {a_feat.max():.3f}]

æ¨¡æ€ç›¸ä¼¼åº¦:
  - å‡å€¼: {similarity.mean():.3f}
  - æ ‡å‡†å·®: {similarity.std():.3f}
  - èŒƒå›´: [{similarity.min():.3f}, {similarity.max():.3f}]
        """
        axes[2, 1].text(0.1, 0.5, stats_text, fontsize=10,
                        verticalalignment='center', family='monospace')

        plt.suptitle(f'æ—¶åºç‰¹å¾æ¼”åŒ–: {name}', fontsize=14, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'features' / f'{name}_temporal.png',
                    dpi=150, bbox_inches='tight')
        plt.close()

        print("  âœ“ æ—¶åºç‰¹å¾å·²ä¿å­˜")

    def _visualize_cava_alignment(self, outputs: Dict, name: str):
        """CAVAå¯¹é½å¯è§†åŒ–"""
        print("\nğŸ¯ 3. CAVAå¯¹é½è¿‡ç¨‹...")

        gate = outputs.get('causal_gate')
        delay = outputs.get('delay_frames')
        v_proj = outputs.get('video_proj')
        a_align = outputs.get('audio_aligned')

        if gate is None:
            print("  âš ï¸  CAVAé—¨æ§æ•°æ®ä¸å¯ç”¨")
            return

        gate = gate[0].cpu().numpy()  # [T] or [T, D]
        if delay is not None:
            delay = delay[0].cpu().item() if delay.dim() == 1 else delay[0].cpu().numpy()

        fig = plt.figure(figsize=(18, 10))
        gs = GridSpec(3, 3, figure=fig)

        # é—¨æ§å€¼æ¼”åŒ–
        ax1 = fig.add_subplot(gs[0, :2])
        if gate.ndim == 1:
            gate_plot = gate
        else:
            gate_plot = gate.mean(axis=1) if gate.ndim > 1 else gate

        T = len(gate_plot)
        ax1.plot(range(T), gate_plot, 'o-', linewidth=2, markersize=8)
        ax1.fill_between(range(T), 0, gate_plot, alpha=0.3)
        ax1.axhline(gate_plot.mean(), linestyle='--', linewidth=2,
                    label=f'Mean={gate_plot.mean():.3f}')
        ax1.set_xlabel('Time step', fontsize=11)
        ax1.set_ylabel('Gate value', fontsize=11)
        ax1.set_title('CAVA Causal Gate Evolution', fontweight='bold', fontsize=12)
        ax1.set_ylim([0, 1])
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å»¶è¿Ÿä¿¡æ¯
        ax2 = fig.add_subplot(gs[0, 2])
        ax2.axis('off')
        if delay is not None:
            delay_val = delay if isinstance(delay, (int, float)) else float(np.mean(delay))
            delay_text = f"""
å»¶è¿Ÿä¼°è®¡:

å€¼: {delay_val:.2f} å¸§

å«ä¹‰:
éŸ³é¢‘ç›¸å¯¹è§†é¢‘
å»¶è¿Ÿçº¦ {delay_val:.1f} å¸§

é—¨æ§ç»Ÿè®¡:
å‡å€¼: {gate_plot.mean():.3f}
æœ€å¤§: {gate_plot.max():.3f}
æœ€å°: {gate_plot.min():.3f}
æ ‡å‡†å·®: {gate_plot.std():.3f}
            """
            ax2.text(0.1, 0.5, delay_text, fontsize=10,
                     verticalalignment='center', family='monospace',
                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        # å¯¹é½å‰åå¯¹æ¯”
        if v_proj is not None and a_align is not None:
            v_proj = v_proj[0].cpu().numpy()  # [T, D]
            a_align = a_align[0].cpu().numpy()

            ax3 = fig.add_subplot(gs[1, :])

            # è®¡ç®—é€å¸§ç›¸ä¼¼åº¦
            T_min = min(v_proj.shape[0], a_align.shape[0])
            v_norm = v_proj[:T_min] / (np.linalg.norm(v_proj[:T_min], axis=1, keepdims=True) + 1e-8)
            a_norm = a_align[:T_min] / (np.linalg.norm(a_align[:T_min], axis=1, keepdims=True) + 1e-8)
            sim = np.sum(v_norm * a_norm, axis=1)

            ax3.plot(range(T_min), sim, 'o-', linewidth=2, label='Alignment similarity')
            ax3.fill_between(range(T_min), 0, sim, alpha=0.3)
            ax3.axhline(sim.mean(), linestyle='--', linewidth=2,
                        label=f'Mean={sim.mean():.3f}')
            ax3.set_xlabel('Time step', fontsize=11)
            ax3.set_ylabel('Cosine Similarity', fontsize=11)
            ax3.set_title('Video-Audio Alignment Quality', fontweight='bold', fontsize=12)
            ax3.legend()
            ax3.grid(True, alpha=0.3)

            # ç›¸å…³çŸ©é˜µ
            ax4 = fig.add_subplot(gs[2, 0])
            corr = np.corrcoef(v_proj[:T_min].T, a_align[:T_min].T)[:v_proj.shape[1], v_proj.shape[1]:]
            im = ax4.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')
            ax4.set_xlabel('Audio features')
            ax4.set_ylabel('Video features')
            ax4.set_title('Cross-modal Correlation', fontweight='bold')
            plt.colorbar(im, ax=ax4, fraction=0.046)

            # ç‰¹å¾è·ç¦»çŸ©é˜µ
            ax5 = fig.add_subplot(gs[2, 1])
            from scipy.spatial.distance import cdist
            dist = cdist(v_proj[:T_min], a_align[:T_min], metric='euclidean')
            im2 = ax5.imshow(dist, cmap='YlOrRd', aspect='auto')
            ax5.set_xlabel('Audio time step')
            ax5.set_ylabel('Video time step')
            ax5.set_title('Temporal Distance Matrix', fontweight='bold')
            plt.colorbar(im2, ax=ax5, fraction=0.046)

            # å¯¹é½æœ‰æ•ˆæ€§æŒ‡æ ‡
            ax6 = fig.add_subplot(gs[2, 2])
            ax6.axis('off')
            alignment_text = f"""
å¯¹é½è´¨é‡è¯„ä¼°:

ç›¸ä¼¼åº¦:
  å‡å€¼: {sim.mean():.3f}
  æœ€å¤§: {sim.max():.3f}
  æœ€å°: {sim.min():.3f}

ç›¸å…³æ€§:
  å‡å€¼: {corr.mean():.3f}
  æœ€å¤§: {corr.max():.3f}

è·ç¦»:
  å‡å€¼: {dist.mean():.2f}
  æœ€å°: {dist.min():.2f}

è¯„ä¼°: {'âœ“ è‰¯å¥½' if sim.mean() > 0.5 else 'âš  ä¸€èˆ¬' if sim.mean() > 0.3 else 'âœ— è¾ƒå·®'}
            """
            ax6.text(0.1, 0.5, alignment_text, fontsize=10,
                     verticalalignment='center', family='monospace',
                     bbox=dict(boxstyle='round', facecolor='lightgreen' if sim.mean() > 0.5 else 'lightyellow',
                               alpha=0.5))

        plt.suptitle(f'CAVAå¯¹é½åˆ†æ: {name}', fontsize=14, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'cava' / f'{name}_alignment.png',
                    dpi=150, bbox_inches='tight')
        plt.close()

        print("  âœ“ CAVAå¯¹é½å·²ä¿å­˜")

    def _visualize_attention_maps(
            self,
            video: torch.Tensor,
            audio: torch.Tensor,
            outputs: Dict,
            name: str
    ):
        """æ³¨æ„åŠ›å›¾å¯è§†åŒ–"""
        print("\nğŸ” 4. æ³¨æ„åŠ›æœºåˆ¶...")

        if not isinstance(outputs, dict):
            print("  âš ï¸  æ— æ³•æå–æ³¨æ„åŠ›ä¿¡æ¯")
            return

        v_feat = outputs.get('video_proj', outputs.get('video_emb'))
        a_feat = outputs.get('audio_aligned', outputs.get('audio_emb'))

        if v_feat is None or a_feat is None:
            print("  âš ï¸  ç¼ºå°‘ç‰¹å¾ç”¨äºæ³¨æ„åŠ›è®¡ç®—")
            return

        v_feat = v_feat[0].cpu().numpy()  # [T, D]
        a_feat = a_feat[0].cpu().numpy()

        T = min(v_feat.shape[0], a_feat.shape[0])

        # è®¡ç®—äº¤å‰æ³¨æ„åŠ›ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
        v_norm = v_feat[:T] / (np.linalg.norm(v_feat[:T], axis=1, keepdims=True) + 1e-8)
        a_norm = a_feat[:T] / (np.linalg.norm(a_feat[:T], axis=1, keepdims=True) + 1e-8)
        attention = np.dot(v_norm, a_norm.T)  # [T, T]

        fig, axes = plt.subplots(2, 2, figsize=(16, 14))

        # äº¤å‰æ³¨æ„åŠ›çŸ©é˜µ
        im1 = axes[0, 0].imshow(attention, cmap='YlOrRd', aspect='auto')
        axes[0, 0].set_xlabel('Audio time step')
        axes[0, 0].set_ylabel('Video time step')
        axes[0, 0].set_title('Cross-modal Attention Matrix', fontweight='bold')
        plt.colorbar(im1, ax=axes[0, 0], fraction=0.046)

        # å¯¹è§’çº¿æ³¨æ„åŠ›ï¼ˆæ—¶åºå¯¹é½ï¼‰
        diag_attention = np.diag(attention)
        axes[0, 1].plot(range(T), diag_attention, 'o-', linewidth=2)
        axes[0, 1].fill_between(range(T), 0, diag_attention, alpha=0.3)
        axes[0, 1].set_xlabel('Time step')
        axes[0, 1].set_ylabel('Attention weight')
        axes[0, 1].set_title('Temporal Alignment Attention', fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)

        # è§†é¢‘è‡ªæ³¨æ„åŠ›
        v_self_attn = np.dot(v_norm, v_norm.T)
        im2 = axes[1, 0].imshow(v_self_attn, cmap='Blues', aspect='auto')
        axes[1, 0].set_xlabel('Video time step')
        axes[1, 0].set_ylabel('Video time step')
        axes[1, 0].set_title('Video Self-Attention', fontweight='bold')
        plt.colorbar(im2, ax=axes[1, 0], fraction=0.046)

        # éŸ³é¢‘è‡ªæ³¨æ„åŠ›
        a_self_attn = np.dot(a_norm, a_norm.T)
        im3 = axes[1, 1].imshow(a_self_attn, cmap='Greens', aspect='auto')
        axes[1, 1].set_xlabel('Audio time step')
        axes[1, 1].set_ylabel('Audio time step')
        axes[1, 1].set_title('Audio Self-Attention', fontweight='bold')
        plt.colorbar(im3, ax=axes[1, 1], fraction=0.046)

        plt.suptitle(f'æ³¨æ„åŠ›åˆ†æ: {name}', fontsize=14, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'attention' / f'{name}_attention.png',
                    dpi=150, bbox_inches='tight')
        plt.close()

        # åˆ›å»ºæ³¨æ„åŠ›çƒ­å›¾çš„åŠ¨ç”»ï¼ˆå¯é€‰ï¼‰
        self._create_attention_animation(attention, name)

        print("  âœ“ æ³¨æ„åŠ›å›¾å·²ä¿å­˜")

    def _create_attention_animation(self, attention: np.ndarray, name: str):
        """åˆ›å»ºæ³¨æ„åŠ›æ¼”åŒ–åŠ¨ç”»"""
        try:
            T = attention.shape[0]

            fig, ax = plt.subplots(figsize=(10, 8))

            def update(frame):
                ax.clear()
                # æ˜¾ç¤ºè¯¥æ—¶é—´æ­¥çš„æ³¨æ„åŠ›åˆ†å¸ƒ
                ax.bar(range(T), attention[frame], alpha=0.7)
                ax.set_xlabel('Attends to time step')
                ax.set_ylabel('Attention weight')
                ax.set_title(f'Attention at time step {frame}', fontweight='bold')
                ax.set_ylim([0, max(1e-8, attention.max() * 1.1)])
                ax.grid(axis='y', alpha=0.3)

            anim = FuncAnimation(fig, update, frames=T, interval=500)
            anim.save(self.output_dir / 'attention' / f'{name}_attention_anim.gif',
                      writer=PillowWriter(fps=2))
            plt.close()

            print("  âœ“ æ³¨æ„åŠ›åŠ¨ç”»å·²ä¿å­˜")
        except Exception as e:
            print(f"  âš ï¸  æ³¨æ„åŠ›åŠ¨ç”»åˆ›å»ºå¤±è´¥: {e}")

    def _visualize_fusion_process(self, outputs: Dict, name: str):
        """èåˆè¿‡ç¨‹å¯è§†åŒ–"""
        print("\nğŸ”€ 5. å¤šæ¨¡æ€èåˆè¿‡ç¨‹...")

        if not isinstance(outputs, dict):
            print("  âš ï¸  æ— æ³•åˆ†æèåˆè¿‡ç¨‹")
            return

        v_feat = outputs.get('video_proj', outputs.get('video_emb'))
        a_feat = outputs.get('audio_aligned', outputs.get('audio_emb'))
        f_feat = outputs.get('fusion_token', outputs.get('fusion_out'))

        if f_feat is None:
            print("  âš ï¸  æ— èåˆç‰¹å¾")
            return

        # æå–ä¸º numpyï¼Œå¹¶å°†ä¸‰è·¯éƒ½é™æˆ 1Dï¼ˆå¯¹æ—¶åºåšå‡å€¼ï¼‰
        def to_1d(x):
            if x is None:
                return None
            arr = x[0].detach().cpu().numpy()
            if arr.ndim > 1:
                arr = arr.mean(axis=0)
            return arr

        v_feat = to_1d(v_feat)
        a_feat = to_1d(a_feat)
        f_feat = to_1d(f_feat)

        # ç»Ÿä¸€å¯¹é½åˆ°æœ€å°ç»´åº¦ï¼ˆæ—¢ç”¨äºç›¸ä¼¼åº¦ï¼Œä¹Ÿç”¨äºå¯è§†åŒ–ä¸ PCAï¼‰
        avail = [x for x in [v_feat, a_feat, f_feat] if x is not None]
        dims = [len(x) for x in avail]
        target_dim = min(dims)

        def align_feature(feat, target):
            if feat is None:
                return None
            if len(feat) > target:
                return feat[:target]
            elif len(feat) < target:
                return np.pad(feat, (0, target - len(feat)), mode='constant')
            else:
                return feat

        v_feat_aligned = align_feature(v_feat, target_dim)
        a_feat_aligned = align_feature(a_feat, target_dim)
        f_feat_aligned = align_feature(f_feat, target_dim)

        if (v_feat is not None and a_feat is not None) and (
            len(v_feat) != len(a_feat) or len(v_feat) != len(f_feat)
        ):
            print(f"  âš ï¸  ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´: Video={len(v_feat)}, Audio={len(a_feat)}, Fusion={len(f_feat)}")
            print(f"     å·²å¯¹é½åˆ° {target_dim} ç»´è¿›è¡Œåˆ†æ")

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # ä»…å½“ä¸‰è·¯éƒ½å¯ç”¨æ—¶ï¼Œç»˜åˆ¶å®Œæ•´åˆ†æ
        if (v_feat_aligned is not None) and (a_feat_aligned is not None):
            # ç›´æ–¹å›¾ï¼ˆç”¨å¯¹é½åçš„å‘é‡ï¼Œä¿è¯ç»Ÿè®¡å¯æ¯”ï¼‰
            axes[0, 0].hist([v_feat_aligned, a_feat_aligned, f_feat_aligned], bins=30,
                            label=['Video', 'Audio', 'Fusion'], alpha=0.6)
            axes[0, 0].set_xlabel('Feature value')
            axes[0, 0].set_ylabel('Frequency')
            axes[0, 0].set_title('Feature Distribution', fontweight='bold')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

            # èŒƒæ•°å¯¹æ¯”ï¼ˆå¯¹é½åï¼‰
            feature_names = ['Video', 'Audio', 'Fusion']
            norms = [np.linalg.norm(v_feat_aligned),
                     np.linalg.norm(a_feat_aligned),
                     np.linalg.norm(f_feat_aligned)]
            axes[0, 1].bar(feature_names, norms, alpha=0.7)
            axes[0, 1].set_ylabel('L2 Norm')
            axes[0, 1].set_title('Feature Magnitude', fontweight='bold')
            axes[0, 1].grid(axis='y', alpha=0.3)

            # ç›¸ä¼¼åº¦åˆ†æï¼ˆä½™å¼¦ï¼‰
            from scipy.spatial.distance import cosine
            try:
                v_a_sim = 1 - cosine(v_feat_aligned, a_feat_aligned)
                v_f_sim = 1 - cosine(v_feat_aligned, f_feat_aligned)
                a_f_sim = 1 - cosine(a_feat_aligned, f_feat_aligned)
            except Exception:
                # é€€åŒ–ä¸ºå½’ä¸€åŒ–ç‚¹ç§¯
                def nz_norm(x):
                    s = np.linalg.norm(x) + 1e-8
                    return x / s
                v_a_sim = float(np.dot(nz_norm(v_feat_aligned), nz_norm(a_feat_aligned)))
                v_f_sim = float(np.dot(nz_norm(v_feat_aligned), nz_norm(f_feat_aligned)))
                a_f_sim = float(np.dot(nz_norm(a_feat_aligned), nz_norm(f_feat_aligned)))

            sim_matrix = np.array([[1, v_a_sim, v_f_sim],
                                   [v_a_sim, 1, a_f_sim],
                                   [v_f_sim, a_f_sim, 1]])

            im = axes[0, 2].imshow(sim_matrix, cmap='RdYlGn', vmin=0, vmax=1)
            axes[0, 2].set_xticks([0, 1, 2])
            axes[0, 2].set_yticks([0, 1, 2])
            axes[0, 2].set_xticklabels(feature_names)
            axes[0, 2].set_yticklabels(feature_names)
            axes[0, 2].set_title('Inter-modality Similarity', fontweight='bold')
            for i in range(3):
                for j in range(3):
                    axes[0, 2].text(j, i, f'{sim_matrix[i, j]:.2f}',
                                    ha="center", va="center", fontsize=12)
            plt.colorbar(im, ax=axes[0, 2], fraction=0.046)

            # PCAï¼ˆ**ä½¿ç”¨å¯¹é½åçš„å‘é‡**ï¼Œé¿å…ç»´åº¦ä¸ä¸€è‡´å¯¼è‡´çš„ np.stack é”™è¯¯ï¼‰
            from sklearn.decomposition import PCA
            combined = np.stack([v_feat_aligned, a_feat_aligned, f_feat_aligned])  # (3, D)
            # n_components ä¸èƒ½è¶…è¿‡æ ·æœ¬æ•°æˆ–ç»´åº¦
            n_comp = 2 if combined.shape[1] >= 2 else 1
            if n_comp >= 1:
                pca = PCA(n_components=n_comp)
                combined_2d = pca.fit_transform(combined)  # (3, n_comp)

                axes[1, 0].scatter(combined_2d[0, 0], combined_2d[0, 1] if n_comp > 1 else 0.0,
                                   s=200, marker='o', label='Video', alpha=0.7)
                axes[1, 0].scatter(combined_2d[1, 0], combined_2d[1, 1] if n_comp > 1 else 0.0,
                                   s=200, marker='s', label='Audio', alpha=0.7)
                axes[1, 0].scatter(combined_2d[2, 0], combined_2d[2, 1] if n_comp > 1 else 0.0,
                                   s=200, marker='*', label='Fusion', alpha=0.7)

                axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})' if n_comp > 1 else 'PC1')
                if n_comp > 1:
                    axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
                else:
                    axes[1, 0].set_ylabel('PC2 (NA)')
                axes[1, 0].set_title('Fusion in PCA Space', fontweight='bold')
                axes[1, 0].legend()
                axes[1, 0].grid(True, alpha=0.3)

            # èåˆæƒé‡ä¼°è®¡ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
            total_sim = (v_f_sim + a_f_sim)
            v_weight = v_f_sim / total_sim if total_sim > 0 else 0.5
            a_weight = a_f_sim / total_sim if total_sim > 0 else 0.5

            axes[1, 1].pie([v_weight, a_weight], labels=['Video', 'Audio'],
                           autopct='%1.1f%%', startangle=90)
            axes[1, 1].set_title('Estimated Modality Weights', fontweight='bold')

            # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨å¯¹é½åçš„ç‰¹å¾ï¼Œç¡®ä¿å¯æ¯”ï¼‰
            axes[1, 2].axis('off')
            fusion_text = f"""
èåˆç»Ÿè®¡:

ç‰¹å¾ç»´åº¦(å¯¹é½å):
  Video: {len(v_feat_aligned)}
  Audio: {len(a_feat_aligned)}
  Fusion: {len(f_feat_aligned)}

ç‰¹å¾èŒƒæ•°:
  Video: {np.linalg.norm(v_feat_aligned):.2f}
  Audio: {np.linalg.norm(a_feat_aligned):.2f}
  Fusion: {np.linalg.norm(f_feat_aligned):.2f}

ç›¸ä¼¼åº¦:
  Video-Audio: {v_a_sim:.3f}
  Video-Fusion: {v_f_sim:.3f}
  Audio-Fusion: {a_f_sim:.3f}

ä¼°è®¡æƒé‡:
  Video: {v_weight:.1%}
  Audio: {a_weight:.1%}
            """
            axes[1, 2].text(0.1, 0.5, fusion_text, fontsize=10,
                            verticalalignment='center', family='monospace',
                            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
        else:
            axes[0, 0].axis('off')
            axes[0, 1].axis('off')
            axes[0, 2].axis('off')
            axes[1, 0].axis('off')
            axes[1, 1].axis('off')
            axes[1, 2].text(0.1, 0.5, "ä»…æœ‰å•ä¸€æ¨¡æ€å¯ç”¨ï¼Œè·³è¿‡èåˆåˆ†æ", fontsize=12)

        plt.suptitle(f'å¤šæ¨¡æ€èåˆåˆ†æ: {name}', fontsize=14, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'fusion' / f'{name}_fusion.png',
                    dpi=150, bbox_inches='tight')
        plt.close()

        print("  âœ“ èåˆåˆ†æå·²ä¿å­˜")

    def _visualize_prediction(
            self,
            probs: np.ndarray,
            pred: int,
            label: Optional[int],
            name: str
    ):
        """é¢„æµ‹ç»“æœå¯è§†åŒ–"""
        print("\nğŸ“Š 6. é¢„æµ‹åˆ†æ...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # Top-5é¢„æµ‹
        top5_idx = np.argsort(probs)[::-1][:5]
        top5_probs = probs[top5_idx]
        top5_names = [self.class_names[i] for i in top5_idx]

        colors = ['green' if i == pred else 'gray' for i in top5_idx]
        axes[0, 0].barh(range(5), top5_probs, color=colors, alpha=0.7)
        axes[0, 0].set_yticks(range(5))
        axes[0, 0].set_yticklabels(top5_names)
        axes[0, 0].set_xlabel('Probability')
        axes[0, 0].set_title('Top-5 Predictions', fontweight='bold')
        axes[0, 0].set_xlim([0, 1])
        axes[0, 0].grid(axis='x', alpha=0.3)

        # æ‰€æœ‰ç±»åˆ«æ¦‚ç‡
        axes[0, 1].bar(range(self.num_classes), probs, alpha=0.7)
        axes[0, 1].bar(pred, probs[pred], color='green', alpha=0.9, label='Predicted')
        if label is not None:
            axes[0, 1].bar(label, probs[label], color='blue', alpha=0.5, label='True')
        axes[0, 1].set_xticks(range(self.num_classes))
        axes[0, 1].set_xticklabels(self.class_names, rotation=45, ha='right')
        axes[0, 1].set_ylabel('Probability')
        axes[0, 1].set_title('All Classes', fontweight='bold')
        axes[0, 1].legend()
        axes[0, 1].grid(axis='y', alpha=0.3)

        # é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ
        pred_entropy = -np.sum(probs * np.log(probs + 1e-10))
        max_entropy = np.log(self.num_classes)
        confidence = probs[pred]

        metrics = {
            'Confidence': confidence,
            'Entropy': pred_entropy / max_entropy,  # å½’ä¸€åŒ–
            'Top-2 Gap': top5_probs[0] - top5_probs[1] if len(top5_probs) > 1 else 0.0,
        }

        axes[1, 0].bar(metrics.keys(), metrics.values(), alpha=0.7)
        axes[1, 0].set_ylabel('Value')
        axes[1, 0].set_title('Prediction Metrics', fontweight='bold')
        axes[1, 0].grid(axis='y', alpha=0.3)
        axes[1, 0].set_ylim([0, 1])

        # é¢„æµ‹æ‘˜è¦
        axes[1, 1].axis('off')
        summary_text = f"""
é¢„æµ‹æ‘˜è¦:

é¢„æµ‹ç±»åˆ«: {self.class_names[pred]}
ç½®ä¿¡åº¦: {confidence:.3f}

"""
        if label is not None:
            summary_text += f"""çœŸå®æ ‡ç­¾: {self.class_names[label]}
ç»“æœ: {'âœ“ æ­£ç¡®' if pred == label else 'âœ— é”™è¯¯'}

"""

        summary_text += f"""Top-5:
  1. {top5_names[0]}: {top5_probs[0]:.3f}
  2. {top5_names[1]}: {top5_probs[1]:.3f}
  3. {top5_names[2]}: {top5_probs[2]:.3f}
  4. {top5_names[3]}: {top5_probs[3]:.3f}
  5. {top5_names[4]}: {top5_probs[4]:.3f}

ä¸ç¡®å®šæ€§:
  ç†µ: {pred_entropy:.3f} / {max_entropy:.3f}
  å½’ä¸€åŒ–ç†µ: {pred_entropy / max_entropy:.3f}
  Top-2å·®è·: {metrics['Top-2 Gap']:.3f}

è¯„ä¼°: {'âœ“ é«˜ç½®ä¿¡' if confidence > 0.8 else 'âš  ä¸­ç­‰ç½®ä¿¡' if confidence > 0.5 else 'âœ— ä½ç½®ä¿¡'}
        """

        color = 'lightgreen' if confidence > 0.8 else 'lightyellow' if confidence > 0.5 else 'lightcoral'
        axes[1, 1].text(0.1, 0.5, summary_text, fontsize=10,
                        verticalalignment='center', family='monospace',
                        bbox=dict(boxstyle='round', facecolor=color, alpha=0.5))

        plt.suptitle(f'é¢„æµ‹åˆ†æ: {name}', fontsize=14, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / f'{name}_prediction.png',
                    dpi=150, bbox_inches='tight')
        plt.close()

        print("  âœ“ é¢„æµ‹åˆ†æå·²ä¿å­˜")

    def _create_summary_figure(
            self,
            video: torch.Tensor,
            audio: torch.Tensor,
            outputs: Dict,
            probs: np.ndarray,
            pred: int,
            label: Optional[int],
            name: str
    ):
        """åˆ›å»ºæ€»ç»“å›¾"""
        print("\nğŸ“‹ 7. ç”Ÿæˆæ€»ç»“å›¾...")

        fig = plt.figure(figsize=(20, 12))
        gs = GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)

        # ç¬¬ä¸€è¡Œï¼šè¾“å…¥æ•°æ®
        video_np = video.cpu().numpy()
        audio_np = audio.cpu().numpy()

        # é€‰æ‹©ä¸­é—´å¸§
        mid_frame = video_np[len(video_np) // 2]
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.imshow(mid_frame.transpose(1, 2, 0))
        ax1.set_title('Input Video Frame', fontweight='bold')
        ax1.axis('off')

        # éŸ³é¢‘å…‰è°±
        ax2 = fig.add_subplot(gs[0, 1])
        audio_mean = audio_np.mean(axis=0)
        im = ax2.imshow(audio_mean, aspect='auto', origin='lower', cmap='viridis')
        ax2.set_title('Audio Spectrogram', fontweight='bold')
        ax2.set_xlabel('Frames')
        ax2.set_ylabel('Mel bins')
        plt.colorbar(im, ax=ax2, fraction=0.046)

        # CAVAé—¨æ§
        ax3 = fig.add_subplot(gs[0, 2:])
        if isinstance(outputs, dict) and 'causal_gate' in outputs:
            gate = outputs['causal_gate'][0].cpu().numpy()
            if gate.ndim > 1:
                gate = gate.mean(axis=1)
            ax3.plot(range(len(gate)), gate, 'o-', linewidth=2)
            ax3.fill_between(range(len(gate)), 0, gate, alpha=0.3)
            ax3.set_title('CAVA Causal Gate', fontweight='bold')
            ax3.set_xlabel('Time step')
            ax3.set_ylabel('Gate value')
            ax3.set_ylim([0, 1])
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, 'CAVA Gate\nNot Available',
                     ha='center', va='center', fontsize=12)
            ax3.axis('off')

        # ç¬¬äºŒè¡Œï¼šç‰¹å¾å’Œç›¸ä¼¼åº¦
        if isinstance(outputs, dict):
            v_feat = outputs.get('video_proj', outputs.get('video_emb'))
            a_feat = outputs.get('audio_aligned', outputs.get('audio_emb'))

            if v_feat is not None:
                ax4 = fig.add_subplot(gs[1, 0])
                v_np = v_feat[0].cpu().numpy()
                im = ax4.imshow(v_np.T, aspect='auto', cmap='Reds')
                ax4.set_title('Video Features', fontweight='bold')
                ax4.set_xlabel('Time')
                ax4.set_ylabel('Dim')
                plt.colorbar(im, ax=ax4, fraction=0.046)

            if a_feat is not None:
                ax5 = fig.add_subplot(gs[1, 1])
                a_np = a_feat[0].cpu().numpy()
                im = ax5.imshow(a_np.T, aspect='auto', cmap='Blues')
                ax5.set_title('Audio Features', fontweight='bold')
                ax5.set_xlabel('Time')
                ax5.set_ylabel('Dim')
                plt.colorbar(im, ax=ax5, fraction=0.046)

            # ç›¸ä¼¼åº¦
            if v_feat is not None and a_feat is not None:
                ax6 = fig.add_subplot(gs[1, 2:])
                v_np = v_feat[0].cpu().numpy()
                a_np = a_feat[0].cpu().numpy()
                T = min(len(v_np), len(a_np))
                v_norm = v_np[:T] / (np.linalg.norm(v_np[:T], axis=1, keepdims=True) + 1e-8)
                a_norm = a_np[:T] / (np.linalg.norm(a_np[:T], axis=1, keepdims=True) + 1e-8)
                sim = np.sum(v_norm * a_norm, axis=1)
                ax6.plot(range(T), sim, 'o-', linewidth=2)
                ax6.fill_between(range(T), 0, sim, alpha=0.3)
                ax6.set_title('Modality Similarity', fontweight='bold')
                ax6.set_xlabel('Time step')
                ax6.set_ylabel('Cosine Similarity')
                ax6.grid(True, alpha=0.3)

        # ç¬¬ä¸‰è¡Œï¼šé¢„æµ‹ç»“æœ
        ax7 = fig.add_subplot(gs[2, :2])
        top5_idx = np.argsort(probs)[::-1][:5]
        top5_probs = probs[top5_idx]
        top5_names = [self.class_names[i] for i in top5_idx]
        colors = ['green' if i == pred else 'gray' for i in top5_idx]
        ax7.barh(range(5), top5_probs, color=colors, alpha=0.7)
        ax7.set_yticks(range(5))
        ax7.set_yticklabels(top5_names)
        ax7.set_xlabel('Probability')
        ax7.set_title('Top-5 Predictions', fontweight='bold')
        ax7.set_xlim([0, 1])
        ax7.grid(axis='x', alpha=0.3)

        # æ‘˜è¦ä¿¡æ¯
        ax8 = fig.add_subplot(gs[2, 2:])
        ax8.axis('off')

        confidence = probs[pred]
        summary = f"""
æ¨ç†æ‘˜è¦

é¢„æµ‹: {self.class_names[pred]}
ç½®ä¿¡åº¦: {confidence:.1%}

"""
        if label is not None:
            summary += f"""çœŸå®: {self.class_names[label]}
{'âœ“ æ­£ç¡®' if pred == label else 'âœ— é”™è¯¯'}

"""

        summary += f"""Top-3:
1. {top5_names[0]}: {top5_probs[0]:.1%}
2. {top5_names[1]}: {top5_probs[1]:.1%}
3. {top5_names[2]}: {top5_probs[2]:.1%}

æ¨¡å‹çŠ¶æ€: æ­£å¸¸
        """

        color = 'lightgreen' if confidence > 0.8 else 'lightyellow'
        ax8.text(0.1, 0.5, summary, fontsize=11,
                 verticalalignment='center', family='monospace',
                 bbox=dict(boxstyle='round', facecolor=color, alpha=0.5))

        plt.suptitle(f'æ¨ç†æ€»ç»“: {name}', fontsize=16, fontweight='bold')
        plt.savefig(self.output_dir / f'{name}_summary.png',
                    dpi=150, bbox_inches='tight')
        plt.close()

        print("  âœ“ æ€»ç»“å›¾å·²ä¿å­˜")


def main():
    parser = argparse.ArgumentParser(description='å•æ ·æœ¬æ¨ç†å¯è§†åŒ–')
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--config', type=str, required=True,
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='./inference_vis',
                        help='è¾“å‡ºç›®å½•')

    # è¾“å…¥æ–¹å¼1ï¼šä»æ•°æ®é›†
    parser.add_argument('--dataset', type=str,
                        help='æ•°æ®é›†CSVæ–‡ä»¶ï¼ˆç”¨äºé€‰æ‹©æ ·æœ¬ï¼‰')
    parser.add_argument('--sample_idx', type=int, default=0,
                        help='æ ·æœ¬ç´¢å¼•')

    # è¾“å…¥æ–¹å¼2ï¼šç›´æ¥æŒ‡å®šæ–‡ä»¶
    parser.add_argument('--video', type=str,
                        help='è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio', type=str,
                        help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("ğŸ¬ å•æ ·æœ¬æ¨ç†å¯è§†åŒ–å·¥å…·")
    print("=" * 60)

    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)

    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ¨¡å‹
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {args.checkpoint}")
    model_cfg = cfg.get("model", {})
    model_cfg["num_classes"] = cfg["data"]["num_classes"]

    model = EnhancedAVTopDetector({
        "model": model_cfg,
        "fusion": model_cfg.get("fusion", {}),
        "cava": cfg.get("cava", {})
    }).to(device)

    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint.get('state_dict', checkpoint), strict=False)
    model.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    # åŠ è½½æ•°æ®
    if args.dataset:
        print(f"ğŸ“Š ä»æ•°æ®é›†åŠ è½½æ ·æœ¬ {args.sample_idx}...")
        dataset = AVFromCSV(
            args.dataset,
            cfg["data"].get("data_root"),
            cfg["data"]["num_classes"],
            cfg["data"]["class_names"],
            cfg.get("video", {}),
            cfg.get("audio", {}),
            is_unlabeled=False
        )

        video, audio, label = dataset[args.sample_idx][:3]
        sample_name = f"sample_{args.sample_idx}"
        label = label.item() if torch.is_tensor(label) else label
    elif args.video and args.audio:
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½...")
        print(f"   è§†é¢‘: {args.video}")
        print(f"   éŸ³é¢‘: {args.audio}")
        # TODO: å¦‚éœ€æ”¯æŒæ–‡ä»¶ç›´è¯»ï¼Œå¯åœ¨æ­¤å®ç°
        print("âŒ æš‚ä¸æ”¯æŒä»æ–‡ä»¶ç›´æ¥åŠ è½½ï¼Œè¯·ä½¿ç”¨ --dataset å‚æ•°")
        return
    else:
        print("âŒ è¯·æŒ‡å®š --dataset æˆ– --video/--audio")
        return

    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = InferenceVisualizer(
        model=model,
        class_names=cfg["data"]["class_names"],
        device=device,
        output_dir=args.output
    )

    # æ‰§è¡Œå¯è§†åŒ–
    visualizer.visualize_sample(video, audio, label, sample_name)

    print("\n" + "=" * 60)
    print("ğŸ‰ å¯è§†åŒ–å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output}")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
